CS 111 Project 2B

Name: Avirudh Theraja
ID: 404459059
TA: Diyu Zhou

Notes about the files ----> 

The deliverables correspond to the spec. There are 2 csv files, one is for lab2b_1.png. This data cannot
be generated again since I didn't bother including the executables from Project 2A. The lab_2b_list.csv file contains data for all
other parts and can be generated again via make tests/graphs. I have avoided some duplicate test cases because I think they are 
redundant. All test cases in the spec are covered and the graphs should make complete sense. The execution profiling report is called
'report' and contains the data generated by pprof for both --text and --list cases. Use make profile to generate it again. The lib/
folder created by gperftools installation is included and so is the pprof executable. There are 2 .gp files, graph1.gp generates
lab2b_1.png and graph2.gp generates all other graphs. lab2b.sh script populates lab_2b_list.csv.

ANSWERS TO THE QUESTIONS ---->

2.3.1.1 
For 1 and/or 2 threads, there is very little contention among the threads since the number of threads is so low. Hence,
most of the cycles are spent executing the code in the critical sections. This is the most expensive part since acquiring locks
is almost instantaneous in both mutex/spin implementations with only 1/2 threads.

2.3.1.2
When the number of threads is high, most of the time is spent by the threads spinning as they are waiting to acquire the lock while only
one thread is executing the code in the critical section at a particular time.

2.3.1.3
Similarly, most of the time in mutex implementations is also being spent in acquiring the lock. We later prove this by looking at the
average wait for lock time.

2.3.2.1
Upon observing the report, we notice that the while loop which calls sync_lock_test_and_set is taking the most time. This is being done
3 times in the thread function and the thread function itself is taking up a huge chunk of the total time of the program.

2.3.2.2
This operation becomes expensive because one thread locks the entire list for any of the 3 operations so with large number of threads,
this doesn't scale well at all. Since only thread operates on the entire list at a time, the other threads are left spinning.

2.3.3.1
The average wait for lock time rises dramatically because our approach of locking the entire list doesn't scale well and a lot of 
threads are just sleeping waiting for the lock to be free. Thus the average time rises dramatically with the number of threads.

2.3.3.2
The completion time's rise is less dramatic because the completion time is dependent on the wait for lock time as well as the time
taken to execute the source code in the critical section. Since the later is helped by multithreading, the rise is less
dramatic but still steady.

2.3.3.3
Because the operation is completed swiftly once the thread acquires the lock. This also implies that the mutex wait time is higher
than the run time which sounds absurd but is possible since we are multithreading after all so some operations are still performed close to parallely thus bringing down the run time. Also the mutex wait time is accumulated for every thread whereas the run time isn't.

2.3.4.1
We see that the throughput increases with the number of lists. This is the case with both mutex and spin lock implementations. This is
because higher lists lead to fewer conflicts as each thread will only lock one list for its operations.

2.3.4.2
I think it should continue increasing since there will be fewer and fewer conflicts with the same number of threads if the lists keep 
increasing until we reach a point where the overhead of creating so many lists starts slowing down the program.

2.3.4.3
No, the throughput for the partitioned list is higher. Thus, a 4 way partioned list with 4 threads is more efficient than 1 list
with 1 thread. This is because partitioning gets rid of the lock wait time and also gives the added benefit of multithreading.

